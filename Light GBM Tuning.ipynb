{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class preparation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.reshape(X.shape[0], X.shape[1] * X.shape[2]) / 255\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Search</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = lgb.Dataset(data = X, label = y)\n",
    "\n",
    "def objective(hyperparameters, iterations):\n",
    "    if 'n_estimators' in hyperparameters.keys():\n",
    "        del hyperparameters['n_estimators']\n",
    "    cross_validation = lgb.cv(hyperparameters, train_set, num_boost_round = 10000,\n",
    "                             nfold = 3, early_stopping_rounds = 80, metrics = 'auc', seed = 3)\n",
    "    score = cross_validation['auc-mean'][-1]\n",
    "    estimator = len(cross_validation['auc-mean'])\n",
    "    hyperparameters['n_estimators'] = estimator\n",
    "    \n",
    "    return [score, hyperparameters, iterations]\n",
    "\n",
    "parameters = {'boosting_type': ['goss', 'gbdt'],\n",
    "             'num_leaves': list(np.arange(40, 200, 10)),\n",
    "             'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "             'min_child_weight': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n",
    "             'reg_alpha': list(np.linspace(0, 1)),\n",
    "             'reg_lambda': list(np.linspace(0,1)),\n",
    "             'subsample': list(np.linspace(0.5, 1, 100)),\n",
    "            'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n",
    "              'is_unbalance': [True, False]}\n",
    "\n",
    "\n",
    "random.seed(3)\n",
    "def random_search(hyperparameters, out_file, iteration):\n",
    "    results = pd.DataFrame(columns = ['Score', 'Hyperparameters', 'Iterations'],\n",
    "                          index = list(range(iteration)))\n",
    "    \n",
    "    for i in range(iteration):\n",
    "        param = {k: random.sample(v, 1)[0] for k, v in hyperparameters.items()}\n",
    "        \n",
    "        param['subsample'] = 1.0 if param['boosting_type'] == 'goss' else param['subsample']\n",
    "\n",
    "        evaluations = objective(param, i)\n",
    "        results.loc[i, :] = evaluations\n",
    "        \n",
    "        of_connection = open(out_file, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow(evaluations)\n",
    "        of_connection.close()\n",
    "        \n",
    "    results.sort_values('Score', ascending = False, inplace = True)\n",
    "    results.reset_index(inplace = True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.315444 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.430763 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332272 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.374355 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.360916 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.351628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.273031 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.302911 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.304091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.397137 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.342046 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.373213 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332375 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.356481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.345246 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.329307 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.338442 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.360598 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.363965 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.374484 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.420261 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.384807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.344039 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.408264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.353946 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.357124 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.357932 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.459249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.332350 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.398037 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.351895 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.368174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.385571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Using GOSS\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.354097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.336964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.344175 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 170881\n",
      "[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 783\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n",
      "[LightGBM] [Info] Start training from score 4.500000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-149-41cb6a99e029>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-148-49cc512f4af2>\u001b[0m in \u001b[0;36mrandom_search\u001b[1;34m(hyperparameters, out_file, iteration)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subsample'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'boosting_type'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'goss'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'subsample'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mevaluations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-148-49cc512f4af2>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(hyperparameters, iterations)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     cross_validation = lgb.cv(hyperparameters, train_set, num_boost_round = 10000,\n\u001b[1;32m----> 7\u001b[1;33m                              nfold = 3, early_stopping_rounds = 80, metrics = 'auc', seed = 3)\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_validation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'auc-mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_validation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'auc-mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, fpreproc, verbose_eval, show_stdv, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[0m\n\u001b[0;32m    590\u001b[0m                                     \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m--> 592\u001b[1;33m         \u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    593\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_agg_cv_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvfolds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_train_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mhandler_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbooster\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboosters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m                 \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    314\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandler_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs_ftmle\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   2370\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[0;32m   2371\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2372\u001b[1;33m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[0;32m   2373\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mFalse\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#results = random_search(parameters, out_file, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.read_csv('Randomized_Parameters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.sort_values(by = 'score', ascending = False, inplace = True)\n",
    "grid.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9089334537037036\n",
      "(\"{'boosting_type': 'goss', 'num_leaves': 80, 'learning_rate': \"\n",
      " \"0.028557932390632156, 'min_child_samples': 60, 'reg_alpha': \"\n",
      " \"0.6122448979591836, 'reg_lambda': 0.8163265306122448, 'subsample': 1.0, \"\n",
      " \"'colsample_bytree': 0.6444444444444444, 'is_unbalance': True, \"\n",
      " \"'n_estimators': 1267}\")\n",
      "0.9082152962962964\n",
      "(\"{'boosting_type': 'gbdt', 'num_leaves': 43, 'learning_rate': \"\n",
      " \"0.007432624224989285, 'subsample_for_bin': 120000, 'min_child_samples': 125, \"\n",
      " \"'reg_alpha': 0.9591836734693877, 'reg_lambda': 0.8571428571428571, \"\n",
      " \"'colsample_bytree': 0.7777777777777778, 'subsample': 0.6616161616161617, \"\n",
      " \"'is_unbalance': True, 'n_estimators': 6943}\")\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(grid.loc[i, 'score'])\n",
    "    pprint.pprint(grid.loc[i , 'hyperparameters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sample 1</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = Pipeline([('prep', preparation()),\n",
    "                           ('pca', PCA(n_components = 300)),\n",
    "                           ('lgb', lgb.LGBMClassifier(boosting_type = 'gbdt', num_leaves = 43, \n",
    "                                                      learning_rate = 0.007432624224989285, \n",
    "                                                      subsample_for_bin = 120000, \n",
    "                                                      min_child_samples = 125, \n",
    "                                                      reg_alpha = 0.9591836734693877, \n",
    "                                                      reg_lambda = 0.8571428571428571, \n",
    "                                                      colsample_bytree = 0.7777777777777778, \n",
    "                                                      subsample = 0.6616161616161617, \n",
    "                                                      is_unbalance = True, \n",
    "                                                      n_estimators = 6943))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8866\n"
     ]
    }
   ],
   "source": [
    "sample_1.fit(X_train, y_train)\n",
    "prediction = sample_1.predict(X_test)\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sample 2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_2 = Pipeline([('prep', preparation()),\n",
    "                           #('pca', PCA(n_components = 300)),\n",
    "                        ('lgb', lgb.LGBMClassifier(boosting_type = 'gbdt', num_leaves = 70, \n",
    "                                                      learning_rate = 0.049884888211816,\n",
    "                                                      min_child_samples = 20, \n",
    "                                                      reg_alpha = 0.4897959183673469, \n",
    "                                                      reg_lambda = 0.5510204081632653, \n",
    "                                                      colsample_bytree = 0.5, \n",
    "                                                      subsample = 0.8888888888888, \n",
    "                                                      is_unbalance = False,\n",
    "                                                      n_estimators = 1477))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9064\n"
     ]
    }
   ],
   "source": [
    "sample_2.fit(X_train, y_train)\n",
    "prediction = sample_2.predict(X_test)\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Sample 3</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_3 = Pipeline([('prep', preparation()),\n",
    "                           #('pca', PCA(n_components = 300)),\n",
    "                           ('lgb', lgb.LGBMClassifier(boosting_type = 'dart', num_leaves = 140, \n",
    "                                                      learning_rate = 0.342614579764203,\n",
    "                                                      min_child_samples = 60, \n",
    "                                                      reg_alpha = 0.9183673469387754, \n",
    "                                                      reg_lambda = 0.6530612244897959, \n",
    "                                                      colsample_bytree = 0.955555555, \n",
    "                                                      subsample = 0.7727272727272727, \n",
    "                                                      is_unbalance = True, \n",
    "                                                      n_estimators = 3500))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9025\n"
     ]
    }
   ],
   "source": [
    "sample_3.fit(X_train, y_train)\n",
    "prediction = sample_3.predict(X_test)\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Self Add Noise </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Preparation</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage, misc\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.copy()\n",
    "X_val = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transform = X.reshape(X.shape[0], -1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For Train Set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame(X_transform)\n",
    "train_label = pd.DataFrame({'Label': y_train})\n",
    "train_full = pd.concat([train_set, train_label], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sample_0 = train_full[train_full['Label'] == 0].sample(750)\n",
    "sample_1 = train_full[train_full['Label'] == 1].sample(750)\n",
    "sample_2 = train_full[train_full['Label'] == 2].sample(750)\n",
    "sample_3 = train_full[train_full['Label'] == 3].sample(750)\n",
    "sample_4 = train_full[train_full['Label'] == 4].sample(750)\n",
    "sample_5 = train_full[train_full['Label'] == 5].sample(750)\n",
    "sample_6 = train_full[train_full['Label'] == 6].sample(750)\n",
    "sample_7 = train_full[train_full['Label'] == 7].sample(750)\n",
    "sample_8 = train_full[train_full['Label'] == 8].sample(750)\n",
    "sample_9 = train_full[train_full['Label'] == 9].sample(750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(x):\n",
    "    rotation = pd.DataFrame([])\n",
    "    for i in range(250):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 90).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    for i in range(250, 500):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 180).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    for i in range(500, 750):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 270).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    return rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "t_0 = rotate(sample_0)\n",
    "t_0['Label'] = 0\n",
    "\n",
    "t_1 = rotate(sample_1)\n",
    "t_1['Label'] = 1\n",
    "\n",
    "t_2 = rotate(sample_2)\n",
    "t_2['Label'] = 2\n",
    "\n",
    "t_3 = rotate(sample_3)\n",
    "t_3['Label'] = 3\n",
    "\n",
    "t_4 = rotate(sample_4)\n",
    "t_4['Label'] = 4\n",
    "\n",
    "t_5 = rotate(sample_5)\n",
    "t_5['Label'] = 5\n",
    "\n",
    "t_6 = rotate(sample_6)\n",
    "t_6['Label'] = 6\n",
    "\n",
    "t_7 = rotate(sample_7)\n",
    "t_7['Label'] = 7\n",
    "\n",
    "t_8 = rotate(sample_8)\n",
    "t_8['Label'] = 8\n",
    "\n",
    "t_9 = rotate(sample_9)\n",
    "t_9['Label'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_train = pd.concat([t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7, t_8, t_9], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((67500, 28, 28), (67500,))"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = np.insert(X, -1, values = rotate_train.iloc[:, :784].values.reshape(-1, 28, 28), axis = 0)\n",
    "y = np.insert(y_train, -1, rotate_train['Label'].values, axis = 0)\n",
    "train_set.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>For Test Set</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_transform = X_val.reshape(X_val.shape[0], -1 )\n",
    "X_val_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.DataFrame(X_val_transform)\n",
    "test_label = pd.DataFrame({'Label': y_test})\n",
    "test_full = pd.concat([test_set, test_label], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_0 = test_full[test_full['Label'] == 0].sample(150)\n",
    "test_1 = test_full[test_full['Label'] == 1].sample(150)\n",
    "test_2 = test_full[test_full['Label'] == 2].sample(150)\n",
    "test_3 = test_full[test_full['Label'] == 3].sample(150)\n",
    "test_4 = test_full[test_full['Label'] == 4].sample(150)\n",
    "test_5 = test_full[test_full['Label'] == 5].sample(150)\n",
    "test_6 = test_full[test_full['Label'] == 6].sample(150)\n",
    "test_7 = test_full[test_full['Label'] == 7].sample(150)\n",
    "test_8 = test_full[test_full['Label'] == 8].sample(150)\n",
    "test_9 = test_full[test_full['Label'] == 9].sample(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_test(x):\n",
    "    rotation = pd.DataFrame([])\n",
    "    for i in range(50):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 90).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    for i in range(50, 100):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 180).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    for i in range(100, 150):\n",
    "        img = x.iloc[i, :784].values.reshape(28, 28)\n",
    "        rot = ndimage.rotate(img, 270).reshape(1, -1)\n",
    "        r = pd.DataFrame(rot,  index = [0])\n",
    "        rotation = rotation.append(r, ignore_index = True)\n",
    "    return rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "te_0 = rotate_test(test_0)\n",
    "te_0['Label'] = 0\n",
    "\n",
    "te_1 = rotate_test(test_1)\n",
    "te_1['Label'] = 1\n",
    "\n",
    "te_2 = rotate_test(test_2)\n",
    "te_2['Label'] = 2\n",
    "\n",
    "te_3 = rotate_test(test_3)\n",
    "te_3['Label'] = 3\n",
    "\n",
    "te_4 = rotate_test(test_4)\n",
    "te_4['Label'] = 4\n",
    "\n",
    "te_5 = rotate_test(test_5)\n",
    "te_5['Label'] = 5\n",
    "\n",
    "te_6 = rotate_test(test_6)\n",
    "te_6['Label'] = 6\n",
    "\n",
    "te_7 = rotate_test(test_7)\n",
    "te_7['Label'] = 7\n",
    "\n",
    "te_8 = rotate_test(test_8)\n",
    "te_8['Label'] = 8\n",
    "\n",
    "te_9 = rotate_test(test_9)\n",
    "te_9['Label'] = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotate_test = pd.concat([te_0, te_1, te_2, te_3, te_4, te_5, te_6, te_7, te_8, te_9], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11500, 28, 28), (11500,))"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = np.insert(X_val, -1, values = rotate_test.iloc[:, :784].values.reshape(-1, 28, 28), axis = 0)\n",
    "y_test = np.insert(y_test, -1, rotate_test['Label'].values, axis = 0)\n",
    "test_set.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>For real</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('prep', preparation()),\n",
    "                           ('pca', PCA(n_components = 350)),\n",
    "                           ('lgb', lgb.LGBMClassifier(boosting_type = 'gbdt', num_leaves = 70, \n",
    "                                                      learning_rate = 0.049884888211816,\n",
    "                                                      min_child_samples = 20, \n",
    "                                                      reg_alpha = 0.4897959183673469, \n",
    "                                                      reg_lambda = 0.5510204081632653, \n",
    "                                                      colsample_bytree = 0.5, \n",
    "                                                      subsample = 0.8888888888888, \n",
    "                                                      is_unbalance = False,\n",
    "                                                      n_estimators = 1477))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8695652173913043\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_set, y)\n",
    "prediction = pipeline.predict(test_set)\n",
    "print(accuracy_score(prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Putting everything together</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "class preparation(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        X = X.reshape(X.shape[0], X.shape[1] * X.shape[2]) / 255\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                ('prep', preparation()),\n",
    "                #('pca', PCA(n_components = 700)),\n",
    "                ('lgb', lgb.LGBMClassifier(boosting_type = 'gbdt', num_leaves = 70, \n",
    "                                                      learning_rate = 0.049884888211816,\n",
    "                                                      min_child_samples = 20, \n",
    "                                                      reg_alpha = 0.4897959183673469, \n",
    "                                                      reg_lambda = 0.5510204081632653, \n",
    "                                                      colsample_bytree = 0.5, \n",
    "                                                      subsample = 0.8888888888888, \n",
    "                                                      is_unbalance = False,\n",
    "                                                      n_estimators = 1477))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((X_train, X_test), axis = 0)\n",
    "y = np.concatenate((y_train, y_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('prep', preparation()), ('pca', PCA(n_components=700)),\n",
       "                ('lgb',\n",
       "                 LGBMClassifier(colsample_bytree=0.5, is_unbalance=False,\n",
       "                                learning_rate=0.049884888211816,\n",
       "                                n_estimators=1477, num_leaves=70,\n",
       "                                reg_alpha=0.4897959183673469,\n",
       "                                reg_lambda=0.5510204081632653,\n",
       "                                subsample=0.8888888888888))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nick.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(pipeline, \"Nick.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('cs_ftmle': conda)",
   "language": "python",
   "name": "python37864bitcsftmleconda8f70978e03294adb93c22517d5f60ab6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
